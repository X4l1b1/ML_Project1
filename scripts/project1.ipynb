{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data input and output paths\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "OUTPUT_PATH = 'predictions_out.csv'\n",
    "\n",
    "# For debugging purpose\n",
    "np.set_printoptions(threshold= 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 38114, 4: 177457, 5: 177457, 6: 177457, 12: 177457, 23: 99913, 24: 99913, 25: 99913, 26: 177457, 27: 177457, 28: 177457}\n"
     ]
    }
   ],
   "source": [
    "# Remove features with unusable data \n",
    "unique, counts = np.unique(np.where(tX < -998)[1], return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "#tX = np.delete(tX, (0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data between -1 and 1\n",
    "tX, mean_x, std_x = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into, training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(330)\n",
    "# How much annotated data for training and validation. The rest is used for testing.\n",
    "training_perc, validation_perc = 0.4, 0.1\n",
    "\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "splits = (np.array([training_perc, validation_perc]) * len(y)).astype(int).cumsum()\n",
    "training_indices, validation_indices, test_indices = np.split(indices, splits)\n",
    "\n",
    "tX_train = tX[training_indices]\n",
    "y_train = y[training_indices]\n",
    "ids_train = ids[training_indices]\n",
    "\n",
    "tX_validation = tX[validation_indices]\n",
    "y_validation = y[validation_indices]\n",
    "ids_validation = ids[validation_indices]\n",
    "\n",
    "tX_test = tX[test_indices]\n",
    "y_test = y[test_indices]\n",
    "ids_test = ids[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=1.0, w0=0.02270010324104557, w1=-0.033357651291556725\n",
      "Gradient Descent(1/499): loss=0.9377066833805242, w0=0.03630256598230802, w1=-0.05605393886293694\n",
      "Gradient Descent(2/499): loss=0.9265100498717618, w0=0.047848655792248014, w1=-0.07555774961931183\n",
      "Gradient Descent(3/499): loss=0.908813501283479, w0=0.05660252841214014, w1=-0.09136396872291716\n",
      "Gradient Descent(4/499): loss=0.8966102970700459, w0=0.06340858402736381, w1=-0.10451256642292625\n",
      "Gradient Descent(5/499): loss=0.8863206563219602, w0=0.06860961069700092, w1=-0.11548963962597862\n",
      "Gradient Descent(6/499): loss=0.8780775459913392, w0=0.072549760234742, w1=-0.12474907179768632\n",
      "Gradient Descent(7/499): loss=0.8713190231616081, w0=0.07548467647511706, w1=-0.13262961948643687\n",
      "Gradient Descent(8/499): loss=0.8657497808325769, w0=0.07762075015108406, w1=-0.13940104505374565\n",
      "Gradient Descent(9/499): loss=0.8611130884137834, w0=0.07912098088162242, w1=-0.145274799551429\n",
      "Gradient Descent(10/499): loss=0.8572106896711577, w0=0.08011520333433922, w1=-0.15041796518106376\n",
      "Gradient Descent(11/499): loss=0.8538942135986811, w0=0.08070682368855045, w1=-0.1549627703239683\n",
      "Gradient Descent(12/499): loss=0.851045296771371, w0=0.08097839642989452, w1=-0.15901425972970162\n",
      "Gradient Descent(13/499): loss=0.8485710814723267, w0=0.08099594051534946, w1=-0.16265618176357793\n",
      "Gradient Descent(14/499): loss=0.8463982613144858, w0=0.08081235041422348, w1=-0.165955578015351\n",
      "Gradient Descent(15/499): loss=0.8444723023554417, w0=0.08047008050205268, w1=-0.16896635048109965\n",
      "Gradient Descent(16/499): loss=0.8427497872135534, w0=0.08000326429238265, w1=-0.1717320412516723\n",
      "Gradient Descent(17/499): loss=0.8411961135700512, w0=0.07943938943567665, w1=-0.17428800101908198\n",
      "Gradient Descent(18/499): loss=0.8397828815976258, w0=0.07880062406901125, w1=-0.17666308377093565\n",
      "Gradient Descent(19/499): loss=0.8384860743432236, w0=0.0781048689640732, w1=-0.1788809735937314\n",
      "Gradient Descent(20/499): loss=0.8372887574387289, w0=0.07736659377653518, w1=-0.18096122556631264\n",
      "Gradient Descent(21/499): loss=0.8361762920093155, w0=0.07659750304301628, w1=-0.18292008417727929\n",
      "Gradient Descent(22/499): loss=0.8351362053643804, w0=0.07580706771480433, w1=-0.18477112839760124\n",
      "Gradient Descent(23/499): loss=0.8341588637805873, w0=0.07500295031823921, w1=-0.18652578148917437\n",
      "Gradient Descent(24/499): loss=0.8332367872615998, w0=0.07419134581639242, w1=-0.18819371509177635\n",
      "Gradient Descent(25/499): loss=0.8323631409070744, w0=0.07337725554196163, w1=-0.189783170529301\n",
      "Gradient Descent(26/499): loss=0.8315318759138908, w0=0.07256470788824299, w1=-0.19130121516914628\n",
      "Gradient Descent(27/499): loss=0.8307386963183452, w0=0.07175693655878328, w1=-0.19275394771519558\n",
      "Gradient Descent(28/499): loss=0.8299799498214563, w0=0.07095652491201772, w1=-0.1941466632521844\n",
      "Gradient Descent(29/499): loss=0.8292522709630441, w0=0.0701655231587777, w1=-0.19548398648475848\n",
      "Gradient Descent(30/499): loss=0.8285532368831167, w0=0.06938554377203311, w1=-0.19676997977193963\n",
      "Gradient Descent(31/499): loss=0.8278802130957291, w0=0.06861783936702352, w1=-0.19800823112646668\n",
      "Gradient Descent(32/499): loss=0.827231023371725, w0=0.06786336644162778, w1=-0.19920192623552524\n",
      "Gradient Descent(33/499): loss=0.8266046425814648, w0=0.06712283768115285, w1=-0.2003539076928999\n",
      "Gradient Descent(34/499): loss=0.8259995458290542, w0=0.06639676498943368, w1=-0.2014667239570857\n",
      "Gradient Descent(35/499): loss=0.8254135958766695, w0=0.06568549497853415, w1=-0.20254267002252282\n",
      "Gradient Descent(36/499): loss=0.8248455074778775, w0=0.06498923830841258, w1=-0.20358382137871395\n",
      "Gradient Descent(37/499): loss=0.8242942843292433, w0=0.06430809399686312, w1=-0.20459206250891926\n",
      "Gradient Descent(38/499): loss=0.8237588082495826, w0=0.06364206960412172, w1=-0.20556911092654648\n",
      "Gradient Descent(39/499): loss=0.8232380268866896, w0=0.06299109802416918, w1=-0.2065165375478962\n",
      "Gradient Descent(40/499): loss=0.8227315277656599, w0=0.06235505147688479, w1=-0.20743578404266913\n",
      "Gradient Descent(41/499): loss=0.8222390946817515, w0=0.061733753184657514, w1=-0.20832817767934841\n",
      "Gradient Descent(42/499): loss=0.8217602225790693, w0=0.061126987128217555, w1=-0.20919494408406072\n",
      "Gradient Descent(43/499): loss=0.8212937658677596, w0=0.06053450620487532, w1=-0.210037218253214\n",
      "Gradient Descent(44/499): loss=0.8208392530811355, w0=0.05995603905453999, w1=-0.21085605409776778\n",
      "Gradient Descent(45/499): loss=0.8203961735201807, w0=0.05939129577206891, w1=-0.21165243274703122\n",
      "Gradient Descent(46/499): loss=0.8199635651664597, w0=0.05883997268647532, w1=-0.21242726979976637\n",
      "Gradient Descent(47/499): loss=0.8195408868408833, w0=0.058301756356557224, w1=-0.2131814216780385\n",
      "Gradient Descent(48/499): loss=0.8191277953687526, w0=0.057776326907219545, w1=-0.21391569121308796\n",
      "Gradient Descent(49/499): loss=0.8187242334474396, w0=0.057263360810046196, w1=-0.21463083257123516\n",
      "Gradient Descent(50/499): loss=0.8183296871387016, w0=0.056762533194658855, w1=-0.21532755561047895\n",
      "Gradient Descent(51/499): loss=0.8179440694231471, w0=0.05627351976337431, w1=-0.21600652974422585\n",
      "Gradient Descent(52/499): loss=0.8175666739600926, w0=0.055795998370080105, w1=-0.21666838737687913\n",
      "Gradient Descent(53/499): loss=0.8171974996534463, w0=0.05532965031463777, w1=-0.21731372696633117\n",
      "Gradient Descent(54/499): loss=0.8168369644806613, w0=0.05487416139613175, w1=-0.2179431157603586\n",
      "Gradient Descent(55/499): loss=0.8164843847387536, w0=0.05442922276161728, w1=-0.21855709224720352\n",
      "Gradient Descent(56/499): loss=0.816139077959944, w0=0.0539945315814461, w1=-0.2191561683549964\n",
      "Gradient Descent(57/499): loss=0.8158008619748489, w0=0.05356979157757268, w1=-0.2197408314299365\n",
      "Gradient Descent(58/499): loss=0.8154697142969209, w0=0.05315471342731076, w1=-0.22031154601913827\n",
      "Gradient Descent(59/499): loss=0.8151454371529138, w0=0.05274901506169217, w1=-0.22086875548065021\n",
      "Gradient Descent(60/499): loss=0.8148274862084506, w0=0.05235242187477573, w1=-0.2214128834402521\n",
      "Gradient Descent(61/499): loss=0.8145158852646294, w0=0.05196466685787574, w1=-0.22194433511215475\n",
      "Gradient Descent(62/499): loss=0.8142106487822319, w0=0.05158549067066003, w1=-0.22246349849859667\n",
      "Gradient Descent(63/499): loss=0.8139115272656388, w0=0.051214641659347304, w1=-0.2229707454814948\n",
      "Gradient Descent(64/499): loss=0.813618191069647, w0=0.05085187583076603, w1=-0.22346643281772036\n",
      "Gradient Descent(65/499): loss=0.8133305709713072, w0=0.05049695678978262, w1=-0.22395090304819568\n",
      "Gradient Descent(66/499): loss=0.8130485340245505, w0=0.05014965564653234, w1=-0.22442448532981102\n",
      "Gradient Descent(67/499): loss=0.8127722085789819, w0=0.04980975089896488, w1=-0.22488749619811918\n",
      "Gradient Descent(68/499): loss=0.8125013313698476, w0=0.049477028295425086, w1=-0.22534024026785493\n",
      "Gradient Descent(69/499): loss=0.8122355500130103, w0=0.049151280681308834, w1=-0.22578301087752944\n",
      "Gradient Descent(70/499): loss=0.8119746470545716, w0=0.04883230783324759, w1=-0.22621609068365023\n",
      "Gradient Descent(71/499): loss=0.8117185326429438, w0=0.04851991628377055, w1=-0.22663975220950305\n",
      "Gradient Descent(72/499): loss=0.811467294005727, w0=0.04821391913895727, w1=-0.22705425835289092\n",
      "Gradient Descent(73/499): loss=0.8112209490728426, w0=0.04791413589121795, w1=-0.22745986285674794\n",
      "Gradient Descent(74/499): loss=0.810979210497347, w0=0.04762039222901391, w1=-0.22785681074612485\n",
      "Gradient Descent(75/499): loss=0.8107420673390734, w0=0.0473325198450504, w1=-0.22824533873466973\n",
      "Gradient Descent(76/499): loss=0.8105092867789776, w0=0.04705035624423188, w1=-0.2286256756033982\n",
      "Gradient Descent(77/499): loss=0.8102806446949741, w0=0.0467737445524606, w1=-0.22899804255425393\n",
      "Gradient Descent(78/499): loss=0.8100561360449331, w0=0.04650253332717904, w1=-0.22936265354070168\n",
      "Gradient Descent(79/499): loss=0.8098357327984713, w0=0.046236576370400795, w1=-0.2297197155773633\n",
      "Gradient Descent(80/499): loss=0.8096192307621978, w0=0.045975732544840164, w1=-0.2300694290305032\n",
      "Gradient Descent(81/499): loss=0.8094066145335137, w0=0.04571986559363503, w1=-0.23041198789098627\n",
      "Gradient Descent(82/499): loss=0.8091976692702953, w0=0.04546884396405798, w1=-0.2307475800311689\n",
      "Gradient Descent(83/499): loss=0.8089925657466522, w0=0.04522254063552505, w1=-0.23107638744703912\n",
      "Gradient Descent(84/499): loss=0.8087910699434189, w0=0.0449808329521381, w1=-0.23139858648679182\n",
      "Gradient Descent(85/499): loss=0.8085929696426994, w0=0.04474360245993404, w1=-0.23171434806690983\n",
      "Gradient Descent(86/499): loss=0.8083982000073084, w0=0.04451073474896042, w1=-0.23202383787671854\n",
      "Gradient Descent(87/499): loss=0.8082065851169074, w0=0.04428211930025143, w1=-0.23232721657228894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(88/499): loss=0.8080182510853164, w0=0.044057649337739556, w1=-0.2326246399604814\n",
      "Gradient Descent(89/499): loss=0.8078331390665411, w0=0.04383722168510552, w1=-0.23291625917384914\n",
      "Gradient Descent(90/499): loss=0.8076514033710676, w0=0.04362073662754168, w1=-0.23320222083705275\n",
      "Gradient Descent(91/499): loss=0.8074728109130509, w0=0.043408097778381276, w1=-0.23348266722537872\n",
      "Gradient Descent(92/499): loss=0.8072973136667079, w0=0.043199211950526735, w1=-0.23375773641590114\n",
      "Gradient Descent(93/499): loss=0.8071248163403261, w0=0.04299398903259496, w1=-0.23402756243177728\n",
      "Gradient Descent(94/499): loss=0.8069553608888469, w0=0.04279234186968466, w1=-0.23429227538012548\n",
      "Gradient Descent(95/499): loss=0.806788798528474, w0=0.04259418614866084, w1=-0.2345520015838944\n",
      "Gradient Descent(96/499): loss=0.806625119515932, w0=0.042399440287843614, w1=-0.2348068637080976\n",
      "Gradient Descent(97/499): loss=0.8064641857797727, w0=0.042208025330982536, w1=-0.23505698088075694\n",
      "Gradient Descent(98/499): loss=0.8063059027360722, w0=0.0420198648453932, w1=-0.23530246880886815\n",
      "Gradient Descent(99/499): loss=0.8061503287161632, w0=0.04183488482412975, w1=-0.23554343988967844\n",
      "Gradient Descent(100/499): loss=0.8059974105724782, w0=0.04165301359206543, w1=-0.23578000331754104\n",
      "Gradient Descent(101/499): loss=0.8058468504587796, w0=0.04147418171575184, w1=-0.2360122651865917\n",
      "Gradient Descent(102/499): loss=0.8056987034610577, w0=0.041298321916928345, w1=-0.23624032858947305\n",
      "Gradient Descent(103/499): loss=0.8055530925486438, w0=0.04112536898955305, w1=-0.23646429371231512\n",
      "Gradient Descent(104/499): loss=0.8054099081026829, w0=0.040955259720228544, w1=-0.23668425792616551\n",
      "Gradient Descent(105/499): loss=0.8052690726257163, w0=0.040787932811897167, w1=-0.2369003158750481\n",
      "Gradient Descent(106/499): loss=0.8051303488256045, w0=0.0406233288106829, w1=-0.23711255956081634\n",
      "Gradient Descent(107/499): loss=0.8049938509418525, w0=0.04046139003575933, w1=-0.23732107842495595\n",
      "Gradient Descent(108/499): loss=0.8048595855864324, w0=0.04030206051212602, w1=-0.23752595942748064\n",
      "Gradient Descent(109/499): loss=0.8047275073538664, w0=0.04014528590617868, w1=-0.2377272871230554\n",
      "Gradient Descent(110/499): loss=0.8045975421340106, w0=0.039991013463961465, w1=-0.23792514373447252\n",
      "Gradient Descent(111/499): loss=0.8044697495840047, w0=0.03983919195199327, w1=-0.23811960922359787\n",
      "Gradient Descent(112/499): loss=0.804343966135998, w0=0.03968977160056285, w1=-0.2383107613598975\n",
      "Gradient Descent(113/499): loss=0.8042201180054317, w0=0.039542704049391364, w1=-0.23849867578664738\n",
      "Gradient Descent(114/499): loss=0.8040983322335501, w0=0.03939794229556411, w1=-0.23868342608492366\n",
      "Gradient Descent(115/499): loss=0.8039784381929783, w0=0.03925544064363666, w1=-0.23886508383546473\n",
      "Gradient Descent(116/499): loss=0.8038603254515834, w0=0.03911515465782394, w1=-0.239043718678491\n",
      "Gradient Descent(117/499): loss=0.8037439511072235, w0=0.03897704111618438, w1=-0.23921939837156353\n",
      "Gradient Descent(118/499): loss=0.8036294512195643, w0=0.03884105796671416, w1=-0.2393921888455588\n",
      "Gradient Descent(119/499): loss=0.8035168463127633, w0=0.03870716428527017, w1=-0.2395621542588316\n",
      "Gradient Descent(120/499): loss=0.8034060471142359, w0=0.0385753202352433, w1=-0.23972935704963558\n",
      "Gradient Descent(121/499): loss=0.8032969778024833, w0=0.03844548702890674, w1=-0.23989385798686622\n",
      "Gradient Descent(122/499): loss=0.8031897155455627, w0=0.0383176268903672, w1=-0.24005571621918867\n",
      "Gradient Descent(123/499): loss=0.8030840470172055, w0=0.03819170302004954, w1=-0.24021498932260946\n",
      "Gradient Descent(124/499): loss=0.802979936103464, w0=0.038067679560648535, w1=-0.24037173334654785\n",
      "Gradient Descent(125/499): loss=0.8028773641953788, w0=0.03794552156448393, w1=-0.24052600285846104\n",
      "Gradient Descent(126/499): loss=0.8027763141949665, w0=0.037825194962197786, w1=-0.2406778509870738\n",
      "Gradient Descent(127/499): loss=0.8026767606458842, w0=0.037706666532735744, w1=-0.2408273294642615\n",
      "Gradient Descent(128/499): loss=0.802578764966925, w0=0.03758990387455605, w1=-0.24097448866563315\n",
      "Gradient Descent(129/499): loss=0.8024823112665546, w0=0.03747487537801288, w1=-0.24111937764985916\n",
      "Gradient Descent(130/499): loss=0.8023873468748978, w0=0.03736155019886275, w1=-0.24126204419678635\n",
      "Gradient Descent(131/499): loss=0.8022938712796014, w0=0.037249898232844754, w1=-0.2414025348443812\n",
      "Gradient Descent(132/499): loss=0.8022017474226241, w0=0.0371398900912879, w1=-0.24154089492454064\n",
      "Gradient Descent(133/499): loss=0.8021109709048998, w0=0.03703149707770061, w1=-0.241677168597808\n",
      "Gradient Descent(134/499): loss=0.8020215772194426, w0=0.036924691165299464, w1=-0.2418113988870302\n",
      "Gradient Descent(135/499): loss=0.8019335433159253, w0=0.036819444975436176, w1=-0.24194362770999114\n",
      "Gradient Descent(136/499): loss=0.8018468659351345, w0=0.0367157317568837, w1=-0.24207389591105424\n",
      "Gradient Descent(137/499): loss=0.8017616106756533, w0=0.03661352536594385, w1=-0.24220224329184692\n",
      "Gradient Descent(138/499): loss=0.8016776395875855, w0=0.03651280024734076, w1=-0.2423287086410173\n",
      "Gradient Descent(139/499): loss=0.8015949466266706, w0=0.03641353141586589, w1=-0.24245332976309336\n",
      "Gradient Descent(140/499): loss=0.8015135520873011, w0=0.03631569443874202, w1=-0.24257614350647289\n",
      "Gradient Descent(141/499): loss=0.8014333004480274, w0=0.03621926541867482, w1=-0.24269718579057217\n",
      "Gradient Descent(142/499): loss=0.8013542799806246, w0=0.036124220977562384, w1=-0.24281649163215993\n",
      "Gradient Descent(143/499): loss=0.8012764719512451, w0=0.03603053824083407, w1=-0.2429340951709022\n",
      "Gradient Descent(144/499): loss=0.8011998014912403, w0=0.03593819482239152, w1=-0.24305002969414297\n",
      "Gradient Descent(145/499): loss=0.8011242460483888, w0=0.035847168810125854, w1=-0.24316432766094478\n",
      "Gradient Descent(146/499): loss=0.8010498932874416, w0=0.035757438751986156, w1=-0.24327702072541194\n",
      "Gradient Descent(147/499): loss=0.8009766902370341, w0=0.035668983642575654, w1=-0.24338813975931908\n",
      "Gradient Descent(148/499): loss=0.8009045332333559, w0=0.0355817829102528, w1=-0.2434977148740666\n",
      "Gradient Descent(149/499): loss=0.8008334412564498, w0=0.035495816404715795, w1=-0.2436057754419836\n",
      "Gradient Descent(150/499): loss=0.8007634284079306, w0=0.03541106438504966, w1=-0.2437123501169988\n",
      "Gradient Descent(151/499): loss=0.8006943356248072, w0=0.03532750750821639, w1=-0.24381746685469852\n",
      "Gradient Descent(152/499): loss=0.8006262610782752, w0=0.035245126817969145, w1=-0.2439211529317912\n",
      "Gradient Descent(153/499): loss=0.8005592345128228, w0=0.03516390373417261, w1=-0.24402343496499604\n",
      "Gradient Descent(154/499): loss=0.8004931891632839, w0=0.035083820042512155, w1=-0.2441243389293737\n",
      "Gradient Descent(155/499): loss=0.8004280970889748, w0=0.03500485788457562, w1=-0.24422389017611632\n",
      "Gradient Descent(156/499): loss=0.800363937271387, w0=0.03492699974829184, w1=-0.2443221134498128\n",
      "Gradient Descent(157/499): loss=0.8003006624055017, w0=0.03485022845871096, w1=-0.24441903290520617\n",
      "Gradient Descent(158/499): loss=0.8002383084580706, w0=0.034774527169112356, w1=-0.2445146721234576\n",
      "Gradient Descent(159/499): loss=0.8001768741454406, w0=0.034699879352426265, w1=-0.2446090541279328\n",
      "Gradient Descent(160/499): loss=0.8001163583076157, w0=0.034626268792956306, w1=-0.24470220139952484\n",
      "Gradient Descent(161/499): loss=0.8000567440762317, w0=0.0345536795783902, w1=-0.24479413589152776\n",
      "Gradient Descent(162/499): loss=0.7999979560569503, w0=0.034482096092086904, w1=-0.2448848790440743\n",
      "Gradient Descent(163/499): loss=0.7999399769088961, w0=0.034411503005628695, w1=-0.2449744517981511\n",
      "Gradient Descent(164/499): loss=0.7998828121784242, w0=0.03434188527162733, w1=-0.24506287460920417\n",
      "Gradient Descent(165/499): loss=0.7998264623858473, w0=0.0342732281167739, w1=-0.24515016746034682\n",
      "Gradient Descent(166/499): loss=0.7997709303588575, w0=0.034205517035122424, w1=-0.24523634987518225\n",
      "Gradient Descent(167/499): loss=0.7997161660854036, w0=0.034138737781597636, w1=-0.2453214409302523\n",
      "Gradient Descent(168/499): loss=0.7996621787429924, w0=0.03407287636571796, w1=-0.24540545926712357\n",
      "Gradient Descent(169/499): loss=0.7996089639500371, w0=0.03400791904552493, w1=-0.2454884231041219\n",
      "Gradient Descent(170/499): loss=0.799556466773929, w0=0.033943852321710793, w1=-0.24557035024772578\n",
      "Gradient Descent(171/499): loss=0.7995047131952108, w0=0.03388066293193631, w1=-0.24565125810362884\n",
      "Gradient Descent(172/499): loss=0.7994537122270426, w0=0.03381833784533126, w1=-0.2457311636874815\n",
      "Gradient Descent(173/499): loss=0.7994034435765843, w0=0.03375686425717026, w1=-0.24581008363532122\n",
      "Gradient Descent(174/499): loss=0.7993538840537389, w0=0.033696229583717104, w1=-0.2458880342137008\n",
      "Gradient Descent(175/499): loss=0.7993050102684498, w0=0.0336364214572309, w1=-0.24596503132952383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(176/499): loss=0.7992568670212331, w0=0.03357742772112768, w1=-0.24604109053959572\n",
      "Gradient Descent(177/499): loss=0.799209394650747, w0=0.033519236425291435, w1=-0.2461162270598994\n",
      "Gradient Descent(178/499): loss=0.7991626138647503, w0=0.03346183582152877, w1=-0.24619045577460336\n",
      "Gradient Descent(179/499): loss=0.7991164986132842, w0=0.033405214359161564, w1=-0.24626379124481015\n",
      "Gradient Descent(180/499): loss=0.7990710060224409, w0=0.033349360680752414, w1=-0.24633624771705348\n",
      "Gradient Descent(181/499): loss=0.7990261636406033, w0=0.03329426361795774, w1=-0.24640783913155084\n",
      "Gradient Descent(182/499): loss=0.7989819593451554, w0=0.03323991218750365, w1=-0.24647857913021928\n",
      "Gradient Descent(183/499): loss=0.7989383875362047, w0=0.03318629558727997, w1=-0.24654848106446142\n",
      "Gradient Descent(184/499): loss=0.798895435600153, w0=0.033133403192547976, w1=-0.24661755800272833\n",
      "Gradient Descent(185/499): loss=0.7988530756149599, w0=0.03308122455225748, w1=-0.24668582273786605\n",
      "Gradient Descent(186/499): loss=0.7988112707970059, w0=0.03302974938546933, w1=-0.24675328779425212\n",
      "Gradient Descent(187/499): loss=0.7987700055012911, w0=0.03297896757787925, w1=-0.24681996543472867\n",
      "Gradient Descent(188/499): loss=0.7987293236048564, w0=0.032928869178439464, w1=-0.24688586766733736\n",
      "Gradient Descent(189/499): loss=0.798689164835594, w0=0.03287944439607432, w1=-0.24695100625186303\n",
      "Gradient Descent(190/499): loss=0.798649506634085, w0=0.03283068359648664, w1=-0.2470153927061909\n",
      "Gradient Descent(191/499): loss=0.7986103642272823, w0=0.0327825772990514, w1=-0.24707903831248346\n",
      "Gradient Descent(192/499): loss=0.7985717303960488, w0=0.03273511617379369, w1=-0.24714195412318188\n",
      "Gradient Descent(193/499): loss=0.7985336676653085, w0=0.032688291038447816, w1=-0.2472041509668376\n",
      "Gradient Descent(194/499): loss=0.7984961302363374, w0=0.03264209285559477, w1=-0.24726563945377872\n",
      "Gradient Descent(195/499): loss=0.7984590713625964, w0=0.03259651272987522, w1=-0.24732642998161655\n",
      "Gradient Descent(196/499): loss=0.7984224962042381, w0=0.03255154190527532, w1=-0.24738653274059644\n",
      "Gradient Descent(197/499): loss=0.7983864345400438, w0=0.03250717176248302, w1=-0.24744595771879804\n",
      "Gradient Descent(198/499): loss=0.7983508654619259, w0=0.03246339381631207, w1=-0.2475047147071892\n",
      "Gradient Descent(199/499): loss=0.7983158033137029, w0=0.03242019971319171, w1=-0.24756281330453778\n",
      "Gradient Descent(200/499): loss=0.7982812011361586, w0=0.032377581228719664, w1=-0.2476202629221856\n",
      "Gradient Descent(201/499): loss=0.7982470411995948, w0=0.03233553026527619, w1=-0.24767707278868886\n",
      "Gradient Descent(202/499): loss=0.7982133403136099, w0=0.03229403884969734, w1=-0.24773325195432835\n",
      "Gradient Descent(203/499): loss=0.7981800783663813, w0=0.03225309913100522, w1=-0.24778880929549404\n",
      "Gradient Descent(204/499): loss=0.7981472575075143, w0=0.03221270337819354, w1=-0.24784375351894714\n",
      "Gradient Descent(205/499): loss=0.7981148627755672, w0=0.03217284397806649, w1=-0.24789809316596345\n",
      "Gradient Descent(206/499): loss=0.7980828719718875, w0=0.03213351343312925, w1=-0.24795183661636178\n",
      "Gradient Descent(207/499): loss=0.7980512778458629, w0=0.03209470435952846, w1=-0.2480049920924201\n",
      "Gradient Descent(208/499): loss=0.7980200919954776, w0=0.032056409485041, w1=-0.24805756766268358\n",
      "Gradient Descent(209/499): loss=0.7979893133936086, w0=0.03201862164710957, w1=-0.24810957124566696\n",
      "Gradient Descent(210/499): loss=0.7979589273347917, w0=0.03198133379092354, w1=-0.2481610106134548\n",
      "Gradient Descent(211/499): loss=0.7979289161744644, w0=0.03194453896754367, w1=-0.24821189339520242\n",
      "Gradient Descent(212/499): loss=0.7978992907433843, w0=0.031908230332069305, w1=-0.24826222708054047\n",
      "Gradient Descent(213/499): loss=0.7978700387007316, w0=0.03187240114184675, w1=-0.2483120190228859\n",
      "Gradient Descent(214/499): loss=0.7978411729889916, w0=0.0318370447547175, w1=-0.2483612764426622\n",
      "Gradient Descent(215/499): loss=0.7978126917562901, w0=0.031802154627305186, w1=-0.24841000643043135\n",
      "Gradient Descent(216/499): loss=0.7977845760732929, w0=0.03176772431333999, w1=-0.24845821594994028\n",
      "Gradient Descent(217/499): loss=0.7977568000521071, w0=0.03173374746201944, w1=-0.2485059118410842\n",
      "Gradient Descent(218/499): loss=0.7977294014577851, w0=0.0317002178164045, w1=-0.24855310082278928\n",
      "Gradient Descent(219/499): loss=0.7977023833926733, w0=0.03166712921184986, w1=-0.24859978949581688\n",
      "Gradient Descent(220/499): loss=0.7976757262566824, w0=0.031634475574467504, w1=-0.24864598434549184\n",
      "Gradient Descent(221/499): loss=0.7976494264862553, w0=0.031602250919622536, w1=-0.24869169174435696\n",
      "Gradient Descent(222/499): loss=0.7976234713953784, w0=0.03157044935046033, w1=-0.24873691795475558\n",
      "Gradient Descent(223/499): loss=0.7975978615610719, w0=0.03153906505646411, w1=-0.24878166913134486\n",
      "Gradient Descent(224/499): loss=0.797572579202501, w0=0.03150809231204216, w1=-0.24882595132354113\n",
      "Gradient Descent(225/499): loss=0.7975476243256385, w0=0.031477525475143715, w1=-0.24886977047789985\n",
      "Gradient Descent(226/499): loss=0.7975229894646512, w0=0.03144735898590283, w1=-0.24891313244043167\n",
      "Gradient Descent(227/499): loss=0.79749868262705, w0=0.03141758736530944, w1=-0.24895604295885665\n",
      "Gradient Descent(228/499): loss=0.7974746942025208, w0=0.03138820521390679, w1=-0.24899850768479836\n",
      "Gradient Descent(229/499): loss=0.7974509995677529, w0=0.031359207210514654, w1=-0.24904053217591965\n",
      "Gradient Descent(230/499): loss=0.7974275904427152, w0=0.03133058811097752, w1=-0.2490821218980016\n",
      "Gradient Descent(231/499): loss=0.7974044752581657, w0=0.03130234274693715, w1=-0.24912328222696756\n",
      "Gradient Descent(232/499): loss=0.7973816539733493, w0=0.03127446602462885, w1=-0.24916401845085356\n",
      "Gradient Descent(233/499): loss=0.7973591317862304, w0=0.031246952923700836, w1=-0.24920433577172693\n",
      "Gradient Descent(234/499): loss=0.7973369151349134, w0=0.031219798496056064, w1=-0.24924423930755432\n",
      "Gradient Descent(235/499): loss=0.79731498584187, w0=0.03119299786471602, w1=-0.2492837340940208\n",
      "Gradient Descent(236/499): loss=0.7972933385180758, w0=0.031166546222705826, w1=-0.24932282508630138\n",
      "Gradient Descent(237/499): loss=0.7972719653795832, w0=0.031140438831960188, w1=-0.24936151716078622\n",
      "Gradient Descent(238/499): loss=0.7972508605141334, w0=0.03111467102224962, w1=-0.24939981511676096\n",
      "Gradient Descent(239/499): loss=0.7972300255809206, w0=0.03108923819012648, w1=-0.24943772367804343\n",
      "Gradient Descent(240/499): loss=0.797209448138914, w0=0.031064135797890285, w1=-0.24947524749457795\n",
      "Gradient Descent(241/499): loss=0.7971891625797045, w0=0.031039359372571906, w1=-0.24951239114398846\n",
      "Gradient Descent(242/499): loss=0.7971691419333677, w0=0.031014904504936085, w1=-0.24954915913309156\n",
      "Gradient Descent(243/499): loss=0.7971493755787886, w0=0.030990766848501956, w1=-0.24958555589937087\n",
      "Gradient Descent(244/499): loss=0.7971298662750604, w0=0.03096694211858103, w1=-0.24962158581241334\n",
      "Gradient Descent(245/499): loss=0.7971105950723485, w0=0.03094342609133232, w1=-0.24965725317530918\n",
      "Gradient Descent(246/499): loss=0.7970915547246158, w0=0.030920214602834162, w1=-0.24969256222601582\n",
      "Gradient Descent(247/499): loss=0.7970727441057865, w0=0.03089730354817235, w1=-0.24972751713868746\n",
      "Gradient Descent(248/499): loss=0.7970541512792737, w0=0.030874688880544215, w1=-0.2497621220249708\n",
      "Gradient Descent(249/499): loss=0.7970357853017698, w0=0.03085236661037831, w1=-0.24979638093526807\n",
      "Gradient Descent(250/499): loss=0.79701763924507, w0=0.030830332804469323, w1=-0.24983029785996835\n",
      "Gradient Descent(251/499): loss=0.7969997232402666, w0=0.03080858358512786, w1=-0.24986387673064783\n",
      "Gradient Descent(252/499): loss=0.7969820274143257, w0=0.030787115129344837, w1=-0.24989712142124018\n",
      "Gradient Descent(253/499): loss=0.7969645638426542, w0=0.03076592366797008, w1=-0.2499300357491776\n",
      "Gradient Descent(254/499): loss=0.7969473272785023, w0=0.030745005484904882, w1=-0.24996262347650358\n",
      "Gradient Descent(255/499): loss=0.7969303077749719, w0=0.030724356916308208, w1=-0.249994888310958\n",
      "Gradient Descent(256/499): loss=0.7969135060300591, w0=0.030703974349816206, w1=-0.2500268339070355\n",
      "Gradient Descent(257/499): loss=0.7968969147847169, w0=0.03068385422377483, w1=-0.25005846386701763\n",
      "Gradient Descent(258/499): loss=0.7968805310053103, w0=0.0306639930264852, w1=-0.25008978174197993\n",
      "Gradient Descent(259/499): loss=0.7968643528159429, w0=0.030644387295461536, w1=-0.25012079103277407\n",
      "Gradient Descent(260/499): loss=0.7968483650549454, w0=0.0306250336167013, w1=-0.2501514951909861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(261/499): loss=0.7968325847120393, w0=0.03060592862396739, w1=-0.25018189761987175\n",
      "Gradient Descent(262/499): loss=0.7968169966115398, w0=0.03058706899808208, w1=-0.2502120016752684\n",
      "Gradient Descent(263/499): loss=0.7968015936752618, w0=0.030568451466232494, w1=-0.25024181066648593\n",
      "Gradient Descent(264/499): loss=0.7967863841181035, w0=0.030550072801287353, w1=-0.25027132785717515\n",
      "Gradient Descent(265/499): loss=0.796771370559227, w0=0.030531929821124855, w1=-0.2503005564661763\n",
      "Gradient Descent(266/499): loss=0.7967565430692988, w0=0.030514019387971315, w1=-0.25032949966834683\n",
      "Gradient Descent(267/499): loss=0.7967418870617392, w0=0.030496338407750526, w1=-0.2503581605953694\n",
      "Gradient Descent(268/499): loss=0.7967273980534373, w0=0.03047888382944352, w1=-0.2503865423365412\n",
      "Gradient Descent(269/499): loss=0.7967130750047582, w0=0.030461652644458544, w1=-0.2504146479395439\n",
      "Gradient Descent(270/499): loss=0.7966989206997447, w0=0.030444641886011103, w1=-0.2504424804111963\n",
      "Gradient Descent(271/499): loss=0.7966849493886898, w0=0.030427848628513825, w1=-0.2504700427181889\n",
      "Gradient Descent(272/499): loss=0.7966711484194231, w0=0.030411269986975967, w1=-0.25049733778780114\n",
      "Gradient Descent(273/499): loss=0.7966575036406722, w0=0.03039490311641244, w1=-0.2505243685086024\n",
      "Gradient Descent(274/499): loss=0.7966440265283878, w0=0.030378745211262083, w1=-0.25055113773113635\n",
      "Gradient Descent(275/499): loss=0.7966307263594596, w0=0.030362793504815102, w1=-0.25057764826858997\n",
      "Gradient Descent(276/499): loss=0.7966175805008583, w0=0.030347045268649436, w1=-0.25060390289744666\n",
      "Gradient Descent(277/499): loss=0.7966045992386005, w0=0.030331497812075926, w1=-0.2506299043581249\n",
      "Gradient Descent(278/499): loss=0.7965917767438795, w0=0.030316148481592115, w1=-0.25065565535560186\n",
      "Gradient Descent(279/499): loss=0.7965791068612867, w0=0.03030099466034452, w1=-0.25068115856002343\n",
      "Gradient Descent(280/499): loss=0.7965665879875792, w0=0.03028603376759924, w1=-0.2507064166072998\n",
      "Gradient Descent(281/499): loss=0.7965542121105755, w0=0.03027126325822072, w1=-0.25073143209968823\n",
      "Gradient Descent(282/499): loss=0.7965419723663044, w0=0.030256680622158538, w1=-0.25075620760636225\n",
      "Gradient Descent(283/499): loss=0.7965298796245482, w0=0.03024228338394211, w1=-0.25078074566396863\n",
      "Gradient Descent(284/499): loss=0.7965179290261145, w0=0.030228069102183105, w1=-0.2508050487771715\n",
      "Gradient Descent(285/499): loss=0.7965061165124658, w0=0.03021403536908551, w1=-0.2508291194191849\n",
      "Gradient Descent(286/499): loss=0.7964944465685219, w0=0.03020017980996313, w1=-0.25085296003229307\n",
      "Gradient Descent(287/499): loss=0.7964829221838471, w0=0.030186500082764457, w1=-0.2508765730283598\n",
      "Gradient Descent(288/499): loss=0.7964715296407457, w0=0.03017299387760479, w1=-0.25089996078932636\n",
      "Gradient Descent(289/499): loss=0.7964602709623599, w0=0.030159658916305402, w1=-0.2509231256676988\n",
      "Gradient Descent(290/499): loss=0.7964491446684175, w0=0.03014649295193972, w1=-0.25094606998702446\n",
      "Gradient Descent(291/499): loss=0.7964381443049714, w0=0.030133493768386338, w1=-0.25096879604235833\n",
      "Gradient Descent(292/499): loss=0.7964272670298972, w0=0.030120659179888824, w1=-0.2509913061007197\n",
      "Gradient Descent(293/499): loss=0.7964165066960953, w0=0.030107987030622074, w1=-0.25101360240153836\n",
      "Gradient Descent(294/499): loss=0.7964058696478737, w0=0.030095475194265267, w1=-0.25103568715709207\n",
      "Gradient Descent(295/499): loss=0.7963953540335962, w0=0.03008312157358117, w1=-0.2510575625529343\n",
      "Gradient Descent(296/499): loss=0.7963849656136547, w0=0.030070924100001777, w1=-0.2510792307483132\n",
      "Gradient Descent(297/499): loss=0.7963747021015211, w0=0.03005888073322014, w1=-0.25110069387658146\n",
      "Gradient Descent(298/499): loss=0.7963645554619055, w0=0.03004698946078829, w1=-0.2511219540455982\n",
      "Gradient Descent(299/499): loss=0.7963545174274769, w0=0.03003524829772115, w1=-0.2511430133381218\n",
      "Gradient Descent(300/499): loss=0.7963445939990866, w0=0.030023655286106378, w1=-0.2511638738121953\n",
      "Gradient Descent(301/499): loss=0.7963347962421988, w0=0.03001220849471996, w1=-0.2511845375015234\n",
      "Gradient Descent(302/499): loss=0.7963251121710258, w0=0.03000090601864757, w1=-0.2512050064158421\n",
      "Gradient Descent(303/499): loss=0.7963155316049494, w0=0.029989745978911478, w1=-0.25122528254128024\n",
      "Gradient Descent(304/499): loss=0.7963060538492557, w0=0.029978726522103042, w1=-0.25124536784071444\n",
      "Gradient Descent(305/499): loss=0.7962966873167521, w0=0.029967845820020624, w1=-0.25126526425411655\n",
      "Gradient Descent(306/499): loss=0.7962874317969817, w0=0.02995710206931282, w1=-0.2512849736988939\n",
      "Gradient Descent(307/499): loss=0.7962782763769677, w0=0.029946493491126994, w1=-0.25130449807022326\n",
      "Gradient Descent(308/499): loss=0.7962692220348042, w0=0.02993601833076297, w1=-0.25132383924137763\n",
      "Gradient Descent(309/499): loss=0.7962602727452038, w0=0.029925674857331837, w1=-0.25134299906404706\n",
      "Gradient Descent(310/499): loss=0.7962514297387788, w0=0.029915461363419753, w1=-0.2513619793686529\n",
      "Gradient Descent(311/499): loss=0.7962426936424487, w0=0.029905376164756702, w1=-0.2513807819646559\n",
      "Gradient Descent(312/499): loss=0.7962340542653169, w0=0.029895417599890107, w1=-0.2513994086408584\n",
      "Gradient Descent(313/499): loss=0.7962255098082949, w0=0.029885584029863216, w1=-0.25141786116570064\n",
      "Gradient Descent(314/499): loss=0.7962170584995316, w0=0.029875873837898227, w1=-0.2514361412875513\n",
      "Gradient Descent(315/499): loss=0.796208702495254, w0=0.02986628542908401, w1=-0.2514542507349928\n",
      "Gradient Descent(316/499): loss=0.796200438615024, w0=0.029856817230068434, w1=-0.2514721912171005\n",
      "Gradient Descent(317/499): loss=0.7961922665431597, w0=0.02984746768875514, w1=-0.2514899644237174\n",
      "Gradient Descent(318/499): loss=0.7961841852328275, w0=0.02983823527400478, w1=-0.25150757202572305\n",
      "Gradient Descent(319/499): loss=0.7961761928603077, w0=0.029829118475340588, w1=-0.25152501567529767\n",
      "Gradient Descent(320/499): loss=0.7961682863093281, w0=0.029820115802658243, w1=-0.25154229700618136\n",
      "Gradient Descent(321/499): loss=0.7961604640026874, w0=0.029811225785939973, w1=-0.2515594176339285\n",
      "Gradient Descent(322/499): loss=0.7961527287429888, w0=0.029802446974972762, w1=-0.25157637915615716\n",
      "Gradient Descent(323/499): loss=0.7961450790532356, w0=0.02979377793907073, w1=-0.2515931831527946\n",
      "Gradient Descent(324/499): loss=0.79613751072207, w0=0.02978521726680146, w1=-0.25160983118631747\n",
      "Gradient Descent(325/499): loss=0.7961300235976656, w0=0.02977676356571636, w1=-0.25162632480198815\n",
      "Gradient Descent(326/499): loss=0.7961226171586328, w0=0.02976841546208492, w1=-0.25164266552808684\n",
      "Gradient Descent(327/499): loss=0.7961152935302633, w0=0.029760171600632768, w1=-0.25165885487613926\n",
      "Gradient Descent(328/499): loss=0.796108049780876, w0=0.0297520306442836, w1=-0.25167489434114027\n",
      "Gradient Descent(329/499): loss=0.796100883725757, w0=0.029743991273904775, w1=-0.25169078540177375\n",
      "Gradient Descent(330/499): loss=0.796093797242854, w0=0.02973605218805661, w1=-0.2517065295206284\n",
      "Gradient Descent(331/499): loss=0.7960867869077004, w0=0.029728212102745316, w1=-0.25172212814440975\n",
      "Gradient Descent(332/499): loss=0.7960798542141088, w0=0.029720469751179472, w1=-0.25173758270414853\n",
      "Gradient Descent(333/499): loss=0.7960730065865694, w0=0.02971282388353004, w1=-0.2517528946154055\n",
      "Gradient Descent(334/499): loss=0.7960662369754222, w0=0.029705273266693803, w1=-0.2517680652784724\n",
      "Gradient Descent(335/499): loss=0.796059543453525, w0=0.029697816684060255, w1=-0.25178309607856986\n",
      "Gradient Descent(336/499): loss=0.7960529268793022, w0=0.029690452935281816, w1=-0.25179798838604167\n",
      "Gradient Descent(337/499): loss=0.7960463821954283, w0=0.029683180836047374, w1=-0.2518127435565457\n",
      "Gradient Descent(338/499): loss=0.7960399064270107, w0=0.029675999217859066, w1=-0.25182736293124186\n",
      "Gradient Descent(339/499): loss=0.7960334975671679, w0=0.0296689069278123, w1=-0.2518418478369766\n",
      "Gradient Descent(340/499): loss=0.7960271564834804, w0=0.02966190282837889, w1=-0.2518561995864644\n",
      "Gradient Descent(341/499): loss=0.7960208848395041, w0=0.029654985797193358, w1=-0.25187041947846633\n",
      "Gradient Descent(342/499): loss=0.7960146838775884, w0=0.029648154726842273, w1=-0.25188450879796576\n",
      "Gradient Descent(343/499): loss=0.7960085514779968, w0=0.029641408524656618, w1=-0.2518984688163406\n",
      "Gradient Descent(344/499): loss=0.7960024831125198, w0=0.029634746112507132, w1=-0.2519123007915335\n",
      "Gradient Descent(345/499): loss=0.7959964787478135, w0=0.02962816642660259, w1=-0.2519260059682187\n",
      "Gradient Descent(346/499): loss=0.7959905394737993, w0=0.02962166841729099, w1=-0.2519395855779664\n",
      "Gradient Descent(347/499): loss=0.7959846651338945, w0=0.029615251048863537, w1=-0.25195304083940456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(348/499): loss=0.7959788554829714, w0=0.02960891329936151, w1=-0.25196637295837776\n",
      "Gradient Descent(349/499): loss=0.7959731112452435, w0=0.029602654160385797, w1=-0.251979583128104\n",
      "Gradient Descent(350/499): loss=0.7959674325778264, w0=0.029596472636909253, w1=-0.2519926725293285\n",
      "Gradient Descent(351/499): loss=0.7959618131719982, w0=0.029590367747091666, w1=-0.2520056423304756\n",
      "Gradient Descent(352/499): loss=0.7959562558833663, w0=0.02958433852209739, w1=-0.25201849368779794\n",
      "Gradient Descent(353/499): loss=0.7959507617232308, w0=0.029578384005915608, w1=-0.2520312277455231\n",
      "Gradient Descent(354/499): loss=0.7959453238937738, w0=0.029572503255183114, w1=-0.25204384563599863\n",
      "Gradient Descent(355/499): loss=0.7959399447829423, w0=0.029566695339009665, w1=-0.25205634847983416\n",
      "Gradient Descent(356/499): loss=0.7959346240137697, w0=0.0295609593388058, w1=-0.2520687373860418\n",
      "Gradient Descent(357/499): loss=0.7959293593245985, w0=0.02955529434811313, w1=-0.252081013452174\n",
      "Gradient Descent(358/499): loss=0.7959241500666082, w0=0.029549699472437036, w1=-0.25209317776445966\n",
      "Gradient Descent(359/499): loss=0.7959189970525802, w0=0.029544173829081744, w1=-0.252105231397938\n",
      "Gradient Descent(360/499): loss=0.7959138972554554, w0=0.029538716546987774, w1=-0.25211717541659034\n",
      "Gradient Descent(361/499): loss=0.7959088510542355, w0=0.029533326766571676, w1=-0.2521290108734701\n",
      "Gradient Descent(362/499): loss=0.7959038571174052, w0=0.029528003639568073, w1=-0.25214073881083077\n",
      "Gradient Descent(363/499): loss=0.7958989159984602, w0=0.029522746328873913, w1=-0.2521523602602519\n",
      "Gradient Descent(364/499): loss=0.7958940270207706, w0=0.02951755400839498, w1=-0.2521638762427633\n",
      "Gradient Descent(365/499): loss=0.795889190469442, w0=0.02951242586289456, w1=-0.2521752877689676\n",
      "Gradient Descent(366/499): loss=0.7958844081676805, w0=0.029507361087844262, w1=-0.2521865958391605\n",
      "Gradient Descent(367/499): loss=0.7958796786531471, w0=0.029502358889276954, w1=-0.25219780144344994\n",
      "Gradient Descent(368/499): loss=0.795874997865477, w0=0.029497418483641794, w1=-0.2522089055618731\n",
      "Gradient Descent(369/499): loss=0.7958703648404554, w0=0.02949253909766132, w1=-0.25221990916451165\n",
      "Gradient Descent(370/499): loss=0.7958657790092731, w0=0.02948771996819058, w1=-0.25223081321160584\n",
      "Gradient Descent(371/499): loss=0.7958612409504489, w0=0.0294829603420782, w1=-0.2522416186536664\n",
      "Gradient Descent(372/499): loss=0.7958567523877333, w0=0.029478259476029525, w1=-0.25225232643158524\n",
      "Gradient Descent(373/499): loss=0.7958523095754736, w0=0.02947361663647159, w1=-0.2522629374767444\n",
      "Gradient Descent(374/499): loss=0.795847913281086, w0=0.029469031099420076, w1=-0.25227345271112345\n",
      "Gradient Descent(375/499): loss=0.7958435628986575, w0=0.02946450215034814, w1=-0.2522838730474056\n",
      "Gradient Descent(376/499): loss=0.7958392567115833, w0=0.029460029084057072, w1=-0.25229419938908215\n",
      "Gradient Descent(377/499): loss=0.795834994820336, w0=0.029455611204548815, w1=-0.25230443263055535\n",
      "Gradient Descent(378/499): loss=0.7958307767230276, w0=0.029451247824900266, w1=-0.25231457365724025\n",
      "Gradient Descent(379/499): loss=0.7958266008030701, w0=0.02944693826713937, w1=-0.2523246233456648\n",
      "Gradient Descent(380/499): loss=0.7958224674155517, w0=0.029442681862122965, w1=-0.2523345825635687\n",
      "Gradient Descent(381/499): loss=0.7958183764264337, w0=0.029438477949416342, w1=-0.2523444521700011\n",
      "Gradient Descent(382/499): loss=0.7958143261566075, w0=0.029434325877174507, w1=-0.2523542330154165\n",
      "Gradient Descent(383/499): loss=0.7958103160128739, w0=0.029430225002025138, w1=-0.2523639259417697\n",
      "Gradient Descent(384/499): loss=0.7958063477294693, w0=0.029426174688953186, w1=-0.25237353178260974\n",
      "Gradient Descent(385/499): loss=0.7958024200713195, w0=0.029422174311187075, w1=-0.2523830513631718\n",
      "Gradient Descent(386/499): loss=0.7957985312907027, w0=0.029418223250086566, w1=-0.2523924855004687\n",
      "Gradient Descent(387/499): loss=0.7957946808502973, w0=0.029414320895032153, w1=-0.25240183500338065\n",
      "Gradient Descent(388/499): loss=0.7957908687554031, w0=0.02941046664331604, w1=-0.25241110067274397\n",
      "Gradient Descent(389/499): loss=0.7957870957719085, w0=0.029406659900034646, w1=-0.25242028330143884\n",
      "Gradient Descent(390/499): loss=0.7957833617687976, w0=0.029402900077982633, w1=-0.2524293836744756\n",
      "Gradient Descent(391/499): loss=0.7957796656432898, w0=0.029399186597548425, w1=-0.25243840256908007\n",
      "Gradient Descent(392/499): loss=0.7957760075094115, w0=0.029395518886611186, w1=-0.25244734075477787\n",
      "Gradient Descent(393/499): loss=0.7957723864979951, w0=0.029391896380439264, w1=-0.2524561989934773\n",
      "Gradient Descent(394/499): loss=0.7957688044104766, w0=0.02938831852159006, w1=-0.25246497803955176\n",
      "Gradient Descent(395/499): loss=0.7957652593032749, w0=0.02938478475981129, w1=-0.25247367863992026\n",
      "Gradient Descent(396/499): loss=0.7957617488776819, w0=0.029381294551943663, w1=-0.2524823015341278\n",
      "Gradient Descent(397/499): loss=0.7957582732997727, w0=0.0293778473618249, w1=-0.2524908474544242\n",
      "Gradient Descent(398/499): loss=0.795754832431906, w0=0.029374442660195108, w1=-0.2524993171258419\n",
      "Gradient Descent(399/499): loss=0.7957514255847915, w0=0.029371079924603503, w1=-0.25250771126627314\n",
      "Gradient Descent(400/499): loss=0.7957480542796098, w0=0.029367758639316417, w1=-0.25251603058654576\n",
      "Gradient Descent(401/499): loss=0.7957447177353285, w0=0.029364478295226617, w1=-0.2525242757904985\n",
      "Gradient Descent(402/499): loss=0.795741413737788, w0=0.029361238389763854, w1=-0.2525324475750549\n",
      "Gradient Descent(403/499): loss=0.7957381419162762, w0=0.029358038426806746, w1=-0.25254054663029646\n",
      "Gradient Descent(404/499): loss=0.7957349019768213, w0=0.0293548779165958, w1=-0.2525485736395352\n",
      "Gradient Descent(405/499): loss=0.7957316943540927, w0=0.029351756375647742, w1=-0.25255652927938477\n",
      "Gradient Descent(406/499): loss=0.7957285192182312, w0=0.029348673326670974, w1=-0.2525644142198312\n",
      "Gradient Descent(407/499): loss=0.7957253750893257, w0=0.02934562829848226, w1=-0.25257222912430244\n",
      "Gradient Descent(408/499): loss=0.7957222624213132, w0=0.029342620825924565, w1=-0.2525799746497371\n",
      "Gradient Descent(409/499): loss=0.7957191807710814, w0=0.029339650449786054, w1=-0.2525876514466525\n",
      "Gradient Descent(410/499): loss=0.7957161290443707, w0=0.029336716716720192, w1=-0.252595260159212\n",
      "Gradient Descent(411/499): loss=0.7957131095701646, w0=0.029333819179166994, w1=-0.2526028014252911\n",
      "Gradient Descent(412/499): loss=0.7957101199146364, w0=0.02933095739527537, w1=-0.25261027587654306\n",
      "Gradient Descent(413/499): loss=0.7957071598042954, w0=0.029328130928826537, w1=-0.2526176841384636\n",
      "Gradient Descent(414/499): loss=0.7957042294451739, w0=0.029325339349158527, w1=-0.252625026830455\n",
      "Gradient Descent(415/499): loss=0.7957013278744901, w0=0.029322582231091732, w1=-0.2526323045658894\n",
      "Gradient Descent(416/499): loss=0.7956984554005473, w0=0.029319859154855483, w1=-0.252639517952171\n",
      "Gradient Descent(417/499): loss=0.7956956130778932, w0=0.029317169706015663, w1=-0.25264666759079796\n",
      "Gradient Descent(418/499): loss=0.7956927998132416, w0=0.029314513475403342, w1=-0.2526537540774235\n",
      "Gradient Descent(419/499): loss=0.7956900135106946, w0=0.02931189005904438, w1=-0.2526607780019161\n",
      "Gradient Descent(420/499): loss=0.7956872546175566, w0=0.029309299058090026, w1=-0.252667739948419\n",
      "Gradient Descent(421/499): loss=0.7956845235508107, w0=0.02930674007874848, w1=-0.2526746404954092\n",
      "Gradient Descent(422/499): loss=0.7956818192049605, w0=0.029304212732217418, w1=-0.25268148021575565\n",
      "Gradient Descent(423/499): loss=0.7956791410736144, w0=0.029301716634617447, w1=-0.2526882596767768\n",
      "Gradient Descent(424/499): loss=0.795676488765238, w0=0.029299251406926467, w1=-0.25269497944029745\n",
      "Gradient Descent(425/499): loss=0.7956738641075565, w0=0.02929681667491499, w1=-0.2527016400627049\n",
      "Gradient Descent(426/499): loss=0.7956712652881376, w0=0.029294412069082315, w1=-0.2527082420950046\n",
      "Gradient Descent(427/499): loss=0.7956686923504956, w0=0.02929203722459363, w1=-0.252714786082875\n",
      "Gradient Descent(428/499): loss=0.7956661449931196, w0=0.029289691781217927, w1=-0.25272127256672194\n",
      "Gradient Descent(429/499): loss=0.7956636218258033, w0=0.029287375383266867, w1=-0.2527277020817323\n",
      "Gradient Descent(430/499): loss=0.7956611232087577, w0=0.02928508767953441, w1=-0.2527340751579269\n",
      "Gradient Descent(431/499): loss=0.7956586497448465, w0=0.029282828323237323, w1=-0.2527403923202132\n",
      "Gradient Descent(432/499): loss=0.7956562002337534, w0=0.029280596971956514, w1=-0.2527466540884372\n",
      "Gradient Descent(433/499): loss=0.7956537744178173, w0=0.02927839328757917, w1=-0.25275286097743443\n",
      "Gradient Descent(434/499): loss=0.795651372187889, w0=0.02927621693624169, w1=-0.25275901349708085\n",
      "Gradient Descent(435/499): loss=0.795648993333437, w0=0.02927406758827342, w1=-0.2527651121523431\n",
      "Gradient Descent(436/499): loss=0.7956466369225657, w0=0.02927194491814118, w1=-0.25277115744332784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(437/499): loss=0.7956443035298031, w0=0.02926984860439451, w1=-0.252777149865331\n",
      "Gradient Descent(438/499): loss=0.7956419928816449, w0=0.029267778329611742, w1=-0.25278308990888604\n",
      "Gradient Descent(439/499): loss=0.7956397042092147, w0=0.02926573378034675, w1=-0.2527889780598121\n",
      "Gradient Descent(440/499): loss=0.7956374370483424, w0=0.02926371464707649, w1=-0.2527948147992613\n",
      "Gradient Descent(441/499): loss=0.795635191176541, w0=0.02926172062414922, w1=-0.2528006006037656\n",
      "Gradient Descent(442/499): loss=0.7956329670256972, w0=0.029259751409733477, w1=-0.2528063359452832\n",
      "Gradient Descent(443/499): loss=0.7956307640305436, w0=0.029257806705767723, w1=-0.25281202129124447\n",
      "Gradient Descent(444/499): loss=0.7956285820826394, w0=0.029255886217910705, w1=-0.2528176571045972\n",
      "Gradient Descent(445/499): loss=0.7956264223532198, w0=0.029253989655492505, w1=-0.2528232438438515\n",
      "Gradient Descent(446/499): loss=0.7956242836670212, w0=0.02925211673146623, w1=-0.2528287819631242\n",
      "Gradient Descent(447/499): loss=0.7956221657416734, w0=0.029250267162360417, w1=-0.25283427191218266\n",
      "Gradient Descent(448/499): loss=0.7956200678269452, w0=0.029248440668232054, w1=-0.2528397141364882\n",
      "Gradient Descent(449/499): loss=0.7956179897095876, w0=0.02924663697262026, w1=-0.2528451090772391\n",
      "Gradient Descent(450/499): loss=0.7956159320323113, w0=0.029244855802500618, w1=-0.25285045717141297\n",
      "Gradient Descent(451/499): loss=0.7956138942561776, w0=0.0292430968882401, w1=-0.2528557588518088\n",
      "Gradient Descent(452/499): loss=0.7956118758376117, w0=0.02924135996355266, w1=-0.25286101454708854\n",
      "Gradient Descent(453/499): loss=0.7956098762159401, w0=0.02923964476545538, w1=-0.2528662246818182\n",
      "Gradient Descent(454/499): loss=0.7956078951386015, w0=0.029237951034225274, w1=-0.2528713896765084\n",
      "Gradient Descent(455/499): loss=0.7956059325094758, w0=0.029236278513356646, w1=-0.25287650994765487\n",
      "Gradient Descent(456/499): loss=0.7956039882574641, w0=0.029234626949519057, w1=-0.25288158590777793\n",
      "Gradient Descent(457/499): loss=0.7956020622616912, w0=0.029232996092515843, w1=-0.2528866179654621\n",
      "Gradient Descent(458/499): loss=0.795600155446915, w0=0.029231385695243226, w1=-0.25289160652539483\n",
      "Gradient Descent(459/499): loss=0.7955982672402232, w0=0.029229795513649973, w1=-0.2528965519884054\n",
      "Gradient Descent(460/499): loss=0.7955963964414347, w0=0.0292282253066976, w1=-0.2529014547515026\n",
      "Gradient Descent(461/499): loss=0.7955945432087115, w0=0.029226674836321156, w1=-0.2529063152079128\n",
      "Gradient Descent(462/499): loss=0.7955927086278565, w0=0.02922514386739048, w1=-0.2529111337471171\n",
      "Gradient Descent(463/499): loss=0.7955908909111062, w0=0.02922363216767206, w1=-0.2529159107548883\n",
      "Gradient Descent(464/499): loss=0.7955890898868523, w0=0.029222139507791355, w1=-0.2529206466133274\n",
      "Gradient Descent(465/499): loss=0.7955873053853586, w0=0.029220665661195664, w1=-0.2529253417008999\n",
      "Gradient Descent(466/499): loss=0.7955855372387368, w0=0.029219210404117495, w1=-0.2529299963924715\n",
      "Gradient Descent(467/499): loss=0.7955837852809269, w0=0.029217773515538416, w1=-0.25293461105934323\n",
      "Gradient Descent(468/499): loss=0.7955820493476753, w0=0.02921635477715344, w1=-0.25293918606928695\n",
      "Gradient Descent(469/499): loss=0.7955803292765137, w0=0.029214953973335848, w1=-0.2529437217865797\n",
      "Gradient Descent(470/499): loss=0.7955786249448449, w0=0.029213570891102516, w1=-0.25294821857203803\n",
      "Gradient Descent(471/499): loss=0.7955769365117535, w0=0.029212205320079725, w1=-0.25295267678305206\n",
      "Gradient Descent(472/499): loss=0.7955752635710123, w0=0.029210857052469404, w1=-0.25295709677361894\n",
      "Gradient Descent(473/499): loss=0.795573605855316, w0=0.02920952588301586, w1=-0.2529614788943761\n",
      "Gradient Descent(474/499): loss=0.7955719633331501, w0=0.029208211608972946, w1=-0.25296582349263425\n",
      "Gradient Descent(475/499): loss=0.7955703367189555, w0=0.029206914030071685, w1=-0.2529701309124098\n",
      "Gradient Descent(476/499): loss=0.7955687249576172, w0=0.02920563294848833, w1=-0.25297440149445705\n",
      "Gradient Descent(477/499): loss=0.7955671284556037, w0=0.029204368168812837, w1=-0.25297863557630035\n",
      "Gradient Descent(478/499): loss=0.7955655465558328, w0=0.029203119498017814, w1=-0.25298283349226525\n",
      "Gradient Descent(479/499): loss=0.7955639789648411, w0=0.02920188674542784, w1=-0.25298699557351\n",
      "Gradient Descent(480/499): loss=0.7955624255386998, w0=0.029200669722689223, w1=-0.2529911221480564\n",
      "Gradient Descent(481/499): loss=0.7955608861350321, w0=0.029199468243740166, w1=-0.2529952135408203\n",
      "Gradient Descent(482/499): loss=0.7955593607263767, w0=0.02919828212478135, w1=-0.25299927007364176\n",
      "Gradient Descent(483/499): loss=0.7955578492817248, w0=0.029197111184246883, w1=-0.2530032920653155\n",
      "Gradient Descent(484/499): loss=0.7955563515488544, w0=0.029195955242775674, w1=-0.25300727983162\n",
      "Gradient Descent(485/499): loss=0.7955548673162743, w0=0.02919481412318316, w1=-0.2530112336853471\n",
      "Gradient Descent(486/499): loss=0.7955533964124145, w0=0.02919368765043347, w1=-0.25301515393633106\n",
      "Gradient Descent(487/499): loss=0.7955519391586606, w0=0.029192575651611898, w1=-0.2530190408914772\n",
      "Gradient Descent(488/499): loss=0.7955504950126204, w0=0.02919147795589778, w1=-0.25302289485479046\n",
      "Gradient Descent(489/499): loss=0.7955490637933158, w0=0.02919039439453776, w1=-0.2530267161274035\n",
      "Gradient Descent(490/499): loss=0.795547645371669, w0=0.029189324800819372, w1=-0.25303050500760477\n",
      "Gradient Descent(491/499): loss=0.7955462396199868, w0=0.02918826901004499, w1=-0.2530342617908657\n",
      "Gradient Descent(492/499): loss=0.7955448464119412, w0=0.02918722685950616, w1=-0.25303798676986844\n",
      "Gradient Descent(493/499): loss=0.7955434656225581, w0=0.029186198188458242, w1=-0.25304168023453266\n",
      "Gradient Descent(494/499): loss=0.7955420971281982, w0=0.0291851828380954, w1=-0.2530453424720424\n",
      "Gradient Descent(495/499): loss=0.7955407408344369, w0=0.029184180651525938, w1=-0.2530489737668725\n",
      "Gradient Descent(496/499): loss=0.7955393966925101, w0=0.029183191473747955, w1=-0.253052574400815\n",
      "Gradient Descent(497/499): loss=0.7955380650679225, w0=0.029182215151625345, w1=-0.25305614465300486\n",
      "Gradient Descent(498/499): loss=0.7955367458635474, w0=0.029181251533864097, w1=-0.2530596847999457\n",
      "Gradient Descent(499/499): loss=0.7955354386948609, w0=0.029180300470988935, w1=-0.2530631951155356\n",
      "Loss:  0.7955354386948609\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 500\n",
    "gamma = 0.1\n",
    "\n",
    "loss, weights = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n",
      "(568238, 30)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "y, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "\n",
    "#tX_test = np.delete(tX_test, (0, 4, 5, 6, 12, 23, 24, 25, 26, 27, 28), 1)\n",
    "\n",
    "print(y.shape)\n",
    "print(tX_test.shape)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n",
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
